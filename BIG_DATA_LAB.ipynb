{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deimiser/DV/blob/main/BIG_DATA_LAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Map Reduce\n",
        "\n",
        "from collections import Counter\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def map_reduce(data):\n",
        "    pool = Pool()\n",
        "    mapped_items = pool.map(Counter, (word for text in data for word in text.split()))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    result = dict(sum(mapped_items, Counter()).items())\n",
        "    return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_data = [\"hello world\", \"world python\", \"python mapreduce\", \"hello python\"]\n",
        "    result = map_reduce(input_data)\n",
        "    print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35xyAjjuzPCx",
        "outputId": "e3eb2a1e-ad25-4b2a-8015-93028bbd3e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'h': 5, 'e': 4, 'l': 6, 'o': 7, 'w': 2, 'r': 3, 'd': 3, 'p': 4, 'y': 3, 't': 3, 'n': 3, 'm': 1, 'a': 1, 'u': 1, 'c': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Bloom filter\n",
        "\n",
        "!pip install pybloom-live\n",
        "\n",
        "from pybloom_live import ScalableBloomFilter\n",
        "\n",
        "# Create a Bloom filter with an estimated error rate of 0.001\n",
        "bloom_filter = ScalableBloomFilter(error_rate=0.001)\n",
        "\n",
        "# Add elements to the Bloom filter\n",
        "elements_to_add = ['apple', 'banana', 'orange']\n",
        "for element in elements_to_add:\n",
        "    bloom_filter.add(element)\n",
        "\n",
        "# Check if an element is in the Bloom filter\n",
        "elements_to_check = ['apple', 'grape', 'banana']\n",
        "\n",
        "for element in elements_to_check:\n",
        "    if element in bloom_filter:\n",
        "        print(f\"{element} may be in the set.\")\n",
        "    else:\n",
        "        print(f\"{element} is definitely not in the set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD98drMd02I4",
        "outputId": "72f794cb-58c8-44d3-9a62-bac551f2e579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybloom-live\n",
            "  Downloading pybloom_live-4.0.0.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitarray>=0.3.4 (from pybloom-live)\n",
            "  Downloading bitarray-2.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (285 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.3/285.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pybloom-live) (3.4.1)\n",
            "Building wheels for collected packages: pybloom-live\n",
            "  Building wheel for pybloom-live (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybloom-live: filename=pybloom_live-4.0.0-py3-none-any.whl size=9227 sha256=bc6af7d96e637f6ebb9e63b9abb3150c97756cd74312b4e0deb9832a5c0eb6d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/8a/9f/83ab00e9a9b2b10cec6135fa2a6cd92a22edf3d87fdaae481e\n",
            "Successfully built pybloom-live\n",
            "Installing collected packages: bitarray, pybloom-live\n",
            "Successfully installed bitarray-2.8.5 pybloom-live-4.0.0\n",
            "apple may be in the set.\n",
            "grape is definitely not in the set.\n",
            "banana may be in the set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Page Ranking\n",
        "import networkx as nx\n",
        "\n",
        "def page_rank(graph):\n",
        "    return nx.pagerank(graph)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace this with your actual graph data\n",
        "    edges = [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A')]\n",
        "\n",
        "    # Create a directed graph\n",
        "    G = nx.DiGraph(edges)\n",
        "\n",
        "    # Calculate PageRank\n",
        "    ranks = page_rank(G)\n",
        "\n",
        "    print(\"PageRank:\", ranks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddJR2Rgv0io3",
        "outputId": "7a6fb40b-0bbc-4ce4-f1de-5b8fae6f28ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PageRank: {'A': 0.43274880303664615, 'B': 0.23391786363002037, 'C': 0.33333333333333326}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title apiriori algo\n",
        "\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "import pandas as pd\n",
        "\n",
        "def get_frequent_itemsets(transactions, min_support):\n",
        "    te = TransactionEncoder()\n",
        "    te_ary = te.fit(transactions).transform(transactions)\n",
        "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "    frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n",
        "    return frequent_itemsets\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    transactions = [\n",
        "        [\"bread\", \"milk\"],\n",
        "        [\"bread\", \"diapers\", \"beer\", \"eggs\"],\n",
        "        [\"milk\", \"diapers\", \"beer\", \"cola\"],\n",
        "        [\"bread\", \"milk\", \"diapers\", \"beer\"],\n",
        "        [\"bread\", \"milk\", \"diapers\", \"cola\"]\n",
        "    ]\n",
        "    min_support = 0.4\n",
        "    frequent_itemsets = get_frequent_itemsets(transactions, min_support)\n",
        "\n",
        "    print(frequent_itemsets)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9shlGEQ2xgC",
        "outputId": "8d9752b9-839e-4526-f14a-fb1e6e899c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    support                itemsets\n",
            "0       0.6                  (beer)\n",
            "1       0.8                 (bread)\n",
            "2       0.4                  (cola)\n",
            "3       0.8               (diapers)\n",
            "4       0.8                  (milk)\n",
            "5       0.4           (beer, bread)\n",
            "6       0.6         (beer, diapers)\n",
            "7       0.4            (milk, beer)\n",
            "8       0.6        (diapers, bread)\n",
            "9       0.6           (milk, bread)\n",
            "10      0.4         (diapers, cola)\n",
            "11      0.4            (milk, cola)\n",
            "12      0.6         (milk, diapers)\n",
            "13      0.4  (beer, diapers, bread)\n",
            "14      0.4   (milk, beer, diapers)\n",
            "15      0.4  (milk, diapers, bread)\n",
            "16      0.4   (milk, diapers, cola)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title HDFS COMMANDS\n",
        "\n",
        "#ONLY WRITE THEM IN THE SHEET . NO NEED TO EXECUTE\n",
        "\n",
        "1.\tCreate a directory in HDFS at given path(s).\n",
        "Usage:\n",
        "\n",
        "hadoop fs -mkdir <paths>\n",
        "\n",
        "2.\tList the contents of a directory.\n",
        "Usage :\n",
        "\n",
        "hadoop fs -ls <args>\n",
        "\n",
        "\n",
        "3.\tUpload and download a file in HDFS.\n",
        "Upload: hadoop fs -put:\n",
        "\n",
        "Download:\n",
        "hadoop fs -get:\n",
        "\n",
        "Copies/Downloads files to the local file system\n",
        "\n",
        "Usage:\n",
        "\n",
        "hadoop fs -get <hdfs_src> <localdst>\n",
        "\n",
        "\n",
        "4.\tSee contents of a file\n",
        "Same as unix cat command:\n",
        "\n",
        "Usage:\n",
        "\n",
        "hadoop fs -cat <path[filename]>\n",
        "\n",
        "5.\tCopy a file from source to destination\n",
        "This command allows multiple sources as well in which case the destination must be a directory.\n",
        "\n",
        "Usage:\n",
        "\n",
        "hadoop fs -cp <source> <dest>\n",
        "\n",
        "6.\tCopy a file from/To Local file system to HDFS\n",
        "copyFromLocal\n",
        "\n",
        "Usage:\n",
        "\n",
        "hadoop fs -copyFromLocal <localsrc> URI\n",
        "\n",
        "Similar to put command, except that the source is restricted to a local file reference.\n",
        "copyToLocal\n",
        "\n",
        "Usage:\n",
        "\n",
        "hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>\n",
        "\n",
        "Similar to get command, except that the destination is restricted to a local file reference.\n",
        "\n",
        "7.\tMove file from source to destination.\n",
        "Note:- Moving files across filesystem is not permitted.\n",
        "\n",
        "Usage :\n",
        "\n",
        "hadoop fs -mv <src> <dest>\n",
        "\n",
        "8.\tRemove a file or directory in HDFS.\n",
        "Remove files specified as argument. Deletes directory only when it is empty\n",
        "\n",
        "Usage :\n",
        "\n",
        "hadoop fs -rm <arg>\n",
        "\n",
        "Recursive version of delete.\n",
        "\n",
        "Usage :\n",
        "\n",
        "hadoop fs -rmr <arg>\n",
        "\n",
        "9.\tDisplay last few lines of a file.\n",
        "Similar to tail command in Unix.\n",
        "\n",
        "Usage :\n",
        "\n",
        "hadoop fs -tail <path[filename]>\n",
        "\n",
        "10.\tDisplay the aggregate length of a file.\n",
        "Usage :\n",
        "\n",
        "hadoop fs -du <path>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KdBDAADe3PJK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}